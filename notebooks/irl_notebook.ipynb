{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rolando/anaconda3/envs/scm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from imitation.policies.serialize import load_policy\n",
    "from imitation.util.util import make_vec_env\n",
    "from imitation.data.wrappers import RolloutInfoWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rolando/anaconda3/envs/scm/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:166: UserWarning: Could not deserialize object learning_rate. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code() argument 13 must be str, not int\n",
      "  warnings.warn(\n",
      "/home/rolando/anaconda3/envs/scm/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:166: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code() argument 13 must be str, not int\n",
      "  warnings.warn(\n",
      "/home/rolando/anaconda3/envs/scm/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:166: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code() argument 13 must be str, not int\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "env = make_vec_env(\n",
    "    \"seals:seals/CartPole-v0\",\n",
    "    rng=np.random.default_rng(),\n",
    "    post_wrappers=[\n",
    "        lambda env, _: RolloutInfoWrapper(env)\n",
    "    ],  # needed for computing rollouts later\n",
    ")\n",
    "expert = load_policy(\n",
    "    \"ppo-huggingface\",\n",
    "    organization=\"HumanCompatibleAI\",\n",
    "    env_name=\"seals/CartPole-v0\",\n",
    "    venv=env,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "reward, _ = evaluate_policy(expert, env, 10)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.data import rollout\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "rollouts = rollout.rollout(\n",
    "    expert,\n",
    "    env,\n",
    "    rollout.make_sample_until(min_timesteps=None, min_episodes=50),\n",
    "    rng=rng,\n",
    ")\n",
    "transitions = rollout.flatten_trajectories(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `rollout` function generated a list of 56 <class 'imitation.data.types.TrajectoryWithRew'>.\n",
      "After flattening, this list is turned into a <class 'imitation.data.types.Transitions'> object containing 28000 transitions.\n",
      "The transitions object contains arrays for: obs, acts, infos, next_obs, dones.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"\"\"The `rollout` function generated a list of {len(rollouts)} {type(rollouts[0])}.\n",
    "After flattening, this list is turned into a {type(transitions)} object containing {len(transitions)} transitions.\n",
    "The transitions object contains arrays for: {', '.join(transitions.__dict__.keys())}.\"\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrajectoryWithRew(obs=array([[ 0.03917111,  0.00851629, -0.00286903,  0.0273277 ],\n",
       "       [ 0.03934143,  0.20367928, -0.00232248, -0.26625904],\n",
       "       [ 0.04341502,  0.00859054, -0.00764766,  0.02569044],\n",
       "       ...,\n",
       "       [ 0.2711443 ,  0.00901818, -0.00818093,  0.01625393],\n",
       "       [ 0.27132466,  0.2042565 , -0.00785585, -0.2789989 ],\n",
       "       [ 0.2754098 ,  0.00924749, -0.01343583,  0.01119598]],\n",
       "      dtype=float32), acts=array([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0]), infos=None, terminal=True, rews=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'obs': array([ 0.03917111,  0.00851629, -0.00286903,  0.0273277 ], dtype=float32),\n",
       " 'acts': 1,\n",
       " 'infos': {},\n",
       " 'next_obs': array([ 0.03934143,  0.20367928, -0.00232248, -0.26625904], dtype=float32),\n",
       " 'dones': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    rng=rng, device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward before training: 9.0\n"
     ]
    }
   ],
   "source": [
    "reward_before_training, _ = evaluate_policy(bc_trainer.policy, env, 10)\n",
    "print(f\"Reward before training: {reward_before_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 0         |\n",
      "|    ent_loss       | -0.000693 |\n",
      "|    entropy        | 0.693     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 72.5      |\n",
      "|    loss           | 0.693     |\n",
      "|    neglogp        | 0.693     |\n",
      "|    prob_true_act  | 0.5       |\n",
      "|    samples_so_far | 32        |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "493batch [00:02, 188.83batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 500       |\n",
      "|    ent_loss       | -0.000376 |\n",
      "|    entropy        | 0.376     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 95.1      |\n",
      "|    loss           | 0.406     |\n",
      "|    neglogp        | 0.407     |\n",
      "|    prob_true_act  | 0.745     |\n",
      "|    samples_so_far | 16032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875batch [00:05, 173.90batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward after training: 500.0\n"
     ]
    }
   ],
   "source": [
    "bc_trainer.train(n_epochs=1)\n",
    "reward_after_training, _ = evaluate_policy(bc_trainer.policy, env, 10)\n",
    "print(f\"Reward after training: {reward_after_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rolando/anaconda3/envs/scm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from imitation.algorithms import preference_comparisons\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "from imitation.policies.base import FeedForward32Policy, NormalizeFeaturesExtractor\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "venv = make_vec_env(\"Pendulum-v1\", rng=rng)\n",
    "\n",
    "reward_net = BasicRewardNet(\n",
    "    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
    ")\n",
    "\n",
    "fragmenter = preference_comparisons.RandomFragmenter(\n",
    "    warning_threshold=0,\n",
    "    rng=rng,\n",
    ")\n",
    "gatherer = preference_comparisons.SyntheticGatherer(rng=rng)\n",
    "preference_model = preference_comparisons.PreferenceModel(reward_net)\n",
    "reward_trainer = preference_comparisons.BasicRewardTrainer(\n",
    "    preference_model=preference_model,\n",
    "    loss=preference_comparisons.CrossEntropyRewardLoss(),\n",
    "    epochs=3,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "\n",
    "# Several hyperparameters (reward_epochs, ppo_clip_range, ppo_ent_coef,\n",
    "# ppo_gae_lambda, ppo_n_epochs, discount_factor, use_sde, sde_sample_freq,\n",
    "# ppo_lr, exploration_frac, num_iterations, initial_comparison_frac,\n",
    "# initial_epoch_multiplier, query_schedule) used in this example have been\n",
    "# approximately fine-tuned to reach a reasonable level of performance.\n",
    "agent = PPO(\n",
    "    policy=FeedForward32Policy,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "    ),\n",
    "    env=venv,\n",
    "    seed=0,\n",
    "    n_steps=2048 // venv.num_envs,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.01,\n",
    "    learning_rate=2e-3,\n",
    "    clip_range=0.1,\n",
    "    gae_lambda=0.95,\n",
    "    gamma=0.97,\n",
    "    n_epochs=10,\n",
    ")\n",
    "\n",
    "trajectory_generator = preference_comparisons.AgentTrainer(\n",
    "    algorithm=agent,\n",
    "    reward_fn=reward_net,\n",
    "    venv=venv,\n",
    "    exploration_frac=0.05,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=5,  # Set to 60 for better performance\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    fragment_length=100,\n",
    "    transition_oversampling=1,\n",
    "    initial_comparison_frac=0.1,\n",
    "    allow_variable_horizon=False,\n",
    "    initial_epoch_multiplier=4,\n",
    "    query_schedule=\"hyperbolic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query schedule: [20, 51, 41, 34, 29, 25]\n",
      "Collecting 40 fragments (4000 transitions)\n",
      "Requested 3800 transitions but only 0 in buffer. Sampling 3800 additional transitions.\n",
      "Sampling 200 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 20 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 12/12 [00:00<00:00, 16.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 1000 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "| raw/                                 |           |\n",
      "|    agent/rollout/ep_len_mean         | 200       |\n",
      "|    agent/rollout/ep_rew_mean         | -1.26e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -37       |\n",
      "|    agent/time/fps                    | 2052      |\n",
      "|    agent/time/iterations             | 1         |\n",
      "|    agent/time/time_elapsed           | 0         |\n",
      "|    agent/time/total_timesteps        | 2048      |\n",
      "----------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| mean/                                   |           |\n",
      "|    agent/rollout/ep_len_mean            | 200       |\n",
      "|    agent/rollout/ep_rew_mean            | -1.26e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean    | -37       |\n",
      "|    agent/time/fps                       | 2.05e+03  |\n",
      "|    agent/time/iterations                | 1         |\n",
      "|    agent/time/time_elapsed              | 0         |\n",
      "|    agent/time/total_timesteps           | 2.05e+03  |\n",
      "|    agent/train/approx_kl                | 0.00125   |\n",
      "|    agent/train/clip_fraction            | 0.053     |\n",
      "|    agent/train/clip_range               | 0.1       |\n",
      "|    agent/train/entropy_loss             | -1.41     |\n",
      "|    agent/train/explained_variance       | -0.381    |\n",
      "|    agent/train/learning_rate            | 0.002     |\n",
      "|    agent/train/loss                     | 0.0701    |\n",
      "|    agent/train/n_updates                | 10        |\n",
      "|    agent/train/policy_gradient_loss     | -0.00176  |\n",
      "|    agent/train/std                      | 0.99      |\n",
      "|    agent/train/value_loss               | 0.209     |\n",
      "|    preferences/entropy                  | 0.0334    |\n",
      "|    reward/epoch-0/train/accuracy        | 0.55      |\n",
      "|    reward/epoch-0/train/gt_reward_loss  | 0.0473    |\n",
      "|    reward/epoch-0/train/loss            | 0.847     |\n",
      "|    reward/epoch-1/train/accuracy        | 0.5       |\n",
      "|    reward/epoch-1/train/gt_reward_loss  | 0.0473    |\n",
      "|    reward/epoch-1/train/loss            | 0.864     |\n",
      "|    reward/epoch-10/train/accuracy       | 1         |\n",
      "|    reward/epoch-10/train/gt_reward_loss | 0.0473    |\n",
      "|    reward/epoch-10/train/loss           | 0.0437    |\n",
      "|    reward/epoch-11/train/accuracy       | 1         |\n",
      "|    reward/epoch-11/train/gt_reward_loss | 0.0473    |\n",
      "|    reward/epoch-11/train/loss           | 0.0372    |\n",
      "|    reward/epoch-2/train/accuracy        | 0.55      |\n",
      "|    reward/epoch-2/train/gt_reward_loss  | 0.0473    |\n",
      "|    reward/epoch-2/train/loss            | 0.586     |\n",
      "|    reward/epoch-3/train/accuracy        | 0.75      |\n",
      "|    reward/epoch-3/train/gt_reward_loss  | 0.0473    |\n",
      "|    reward/epoch-3/train/loss            | 0.407     |\n",
      "|    reward/epoch-4/train/accuracy        | 0.8       |\n",
      "|    reward/epoch-4/train/gt_reward_loss  | 0.0473    |\n",
      "|    reward/epoch-4/train/loss            | 0.269     |\n",
      "|    reward/epoch-5/train/accuracy        | 1         |\n",
      "|    reward/epoch-5/train/gt_reward_loss  | 0.0473    |\n",
      "|    reward/epoch-5/train/loss            | 0.174     |\n",
      "|    reward/epoch-6/train/accuracy        | 1         |\n",
      "|    reward/epoch-6/train/gt_reward_loss  | 0.0473    |\n",
      "|    reward/epoch-6/train/loss            | 0.121     |\n",
      "|    reward/epoch-7/train/accuracy        | 1         |\n",
      "|    reward/epoch-7/train/gt_reward_loss  | 0.0473    |\n",
      "|    reward/epoch-7/train/loss            | 0.089     |\n",
      "|    reward/epoch-8/train/accuracy        | 1         |\n",
      "|    reward/epoch-8/train/gt_reward_loss  | 0.0473    |\n",
      "|    reward/epoch-8/train/loss            | 0.0668    |\n",
      "|    reward/epoch-9/train/accuracy        | 1         |\n",
      "|    reward/epoch-9/train/gt_reward_loss  | 0.0473    |\n",
      "|    reward/epoch-9/train/loss            | 0.0531    |\n",
      "| reward/                                 |           |\n",
      "|    final/train/accuracy                 | 1         |\n",
      "|    final/train/gt_reward_loss           | 0.0473    |\n",
      "|    final/train/loss                     | 0.0372    |\n",
      "-------------------------------------------------------\n",
      "Collecting 102 fragments (10200 transitions)\n",
      "Requested 9690 transitions but only 1600 in buffer. Sampling 8090 additional transitions.\n",
      "Sampling 510 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 71 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [00:00<00:00,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 1000 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.17e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -20          |\n",
      "|    agent/time/fps                    | 2486         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 4096         |\n",
      "|    agent/train/approx_kl             | 0.0012476778 |\n",
      "|    agent/train/clip_fraction         | 0.053        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.41        |\n",
      "|    agent/train/explained_variance    | -0.381       |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0701       |\n",
      "|    agent/train/n_updates             | 10           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00176     |\n",
      "|    agent/train/std                   | 0.99         |\n",
      "|    agent/train/value_loss            | 0.209        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.17e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -20       |\n",
      "|    agent/time/fps                      | 2.49e+03  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 4.1e+03   |\n",
      "|    agent/train/approx_kl               | 0.0019    |\n",
      "|    agent/train/clip_fraction           | 0.0988    |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.41     |\n",
      "|    agent/train/explained_variance      | 0.662     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.0565    |\n",
      "|    agent/train/n_updates               | 20        |\n",
      "|    agent/train/policy_gradient_loss    | -0.00491  |\n",
      "|    agent/train/std                     | 1         |\n",
      "|    agent/train/value_loss              | 0.213     |\n",
      "|    preferences/entropy                 | 1.55e-05  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.979     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00986   |\n",
      "|    reward/epoch-0/train/loss           | 0.0569    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.979     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00986   |\n",
      "|    reward/epoch-1/train/loss           | 0.0504    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.979     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00986   |\n",
      "|    reward/epoch-2/train/loss           | 0.0402    |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.979     |\n",
      "|    final/train/gt_reward_loss          | 0.00986   |\n",
      "|    final/train/loss                    | 0.0402    |\n",
      "------------------------------------------------------\n",
      "Collecting 82 fragments (8200 transitions)\n",
      "Requested 7790 transitions but only 1600 in buffer. Sampling 6190 additional transitions.\n",
      "Sampling 410 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 112 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [00:00<00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 1000 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.16e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.32        |\n",
      "|    agent/time/fps                    | 2375         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 6144         |\n",
      "|    agent/train/approx_kl             | 0.0019020251 |\n",
      "|    agent/train/clip_fraction         | 0.0988       |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.41        |\n",
      "|    agent/train/explained_variance    | 0.662        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0565       |\n",
      "|    agent/train/n_updates             | 20           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00491     |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.213        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.16e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -8.32     |\n",
      "|    agent/time/fps                      | 2.38e+03  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 6.14e+03  |\n",
      "|    agent/train/approx_kl               | 0.00219   |\n",
      "|    agent/train/clip_fraction           | 0.104     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.41     |\n",
      "|    agent/train/explained_variance      | 0.894     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.0185    |\n",
      "|    agent/train/n_updates               | 30        |\n",
      "|    agent/train/policy_gradient_loss    | -0.00544  |\n",
      "|    agent/train/std                     | 0.985     |\n",
      "|    agent/train/value_loss              | 0.125     |\n",
      "|    preferences/entropy                 | 0.000159  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.938     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0074    |\n",
      "|    reward/epoch-0/train/loss           | 0.0892    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.945     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0074    |\n",
      "|    reward/epoch-1/train/loss           | 0.096     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.953     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0074    |\n",
      "|    reward/epoch-2/train/loss           | 0.0735    |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.953     |\n",
      "|    final/train/gt_reward_loss          | 0.0074    |\n",
      "|    final/train/loss                    | 0.0735    |\n",
      "------------------------------------------------------\n",
      "Collecting 68 fragments (6800 transitions)\n",
      "Requested 6460 transitions but only 1600 in buffer. Sampling 4860 additional transitions.\n",
      "Sampling 340 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 146 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [00:01<00:00,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 1000 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.17e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.91        |\n",
      "|    agent/time/fps                    | 2387         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 8192         |\n",
      "|    agent/train/approx_kl             | 0.0021893103 |\n",
      "|    agent/train/clip_fraction         | 0.104        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.41        |\n",
      "|    agent/train/explained_variance    | 0.894        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0185       |\n",
      "|    agent/train/n_updates             | 30           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00544     |\n",
      "|    agent/train/std                   | 0.985        |\n",
      "|    agent/train/value_loss            | 0.125        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.17e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -3.91     |\n",
      "|    agent/time/fps                      | 2.39e+03  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 8.19e+03  |\n",
      "|    agent/train/approx_kl               | 0.00237   |\n",
      "|    agent/train/clip_fraction           | 0.136     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.41     |\n",
      "|    agent/train/explained_variance      | 0.93      |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.0397    |\n",
      "|    agent/train/n_updates               | 40        |\n",
      "|    agent/train/policy_gradient_loss    | -0.00766  |\n",
      "|    agent/train/std                     | 0.993     |\n",
      "|    agent/train/value_loss              | 0.128     |\n",
      "|    preferences/entropy                 | 0.00036   |\n",
      "|    reward/epoch-0/train/accuracy       | 0.913     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00593   |\n",
      "|    reward/epoch-0/train/loss           | 0.211     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.939     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00593   |\n",
      "|    reward/epoch-1/train/loss           | 0.147     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.95      |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00593   |\n",
      "|    reward/epoch-2/train/loss           | 0.125     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.95      |\n",
      "|    final/train/gt_reward_loss          | 0.00593   |\n",
      "|    final/train/loss                    | 0.125     |\n",
      "------------------------------------------------------\n",
      "Collecting 58 fragments (5800 transitions)\n",
      "Requested 5510 transitions but only 1600 in buffer. Sampling 3910 additional transitions.\n",
      "Sampling 290 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 175 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [00:01<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 1000 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.18e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.17        |\n",
      "|    agent/time/fps                    | 1613         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 1            |\n",
      "|    agent/time/total_timesteps        | 10240        |\n",
      "|    agent/train/approx_kl             | 0.0023718174 |\n",
      "|    agent/train/clip_fraction         | 0.136        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.41        |\n",
      "|    agent/train/explained_variance    | 0.93         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0397       |\n",
      "|    agent/train/n_updates             | 40           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00766     |\n",
      "|    agent/train/std                   | 0.993        |\n",
      "|    agent/train/value_loss            | 0.128        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.18e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -4.17     |\n",
      "|    agent/time/fps                      | 1.61e+03  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 1         |\n",
      "|    agent/time/total_timesteps          | 1.02e+04  |\n",
      "|    agent/train/approx_kl               | 0.00417   |\n",
      "|    agent/train/clip_fraction           | 0.194     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.39     |\n",
      "|    agent/train/explained_variance      | 0.967     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | -0.00442  |\n",
      "|    agent/train/n_updates               | 50        |\n",
      "|    agent/train/policy_gradient_loss    | -0.0122   |\n",
      "|    agent/train/std                     | 0.978     |\n",
      "|    agent/train/value_loss              | 0.124     |\n",
      "|    preferences/entropy                 | 0.000126  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.947     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00495   |\n",
      "|    reward/epoch-0/train/loss           | 0.154     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.974     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00494   |\n",
      "|    reward/epoch-1/train/loss           | 0.12      |\n",
      "|    reward/epoch-2/train/accuracy       | 0.964     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00494   |\n",
      "|    reward/epoch-2/train/loss           | 0.112     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.964     |\n",
      "|    final/train/gt_reward_loss          | 0.00494   |\n",
      "|    final/train/loss                    | 0.112     |\n",
      "------------------------------------------------------\n",
      "Collecting 50 fragments (5000 transitions)\n",
      "Requested 4750 transitions but only 1600 in buffer. Sampling 3150 additional transitions.\n",
      "Sampling 250 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 200 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [00:01<00:00,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 1000 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.19e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.74        |\n",
      "|    agent/time/fps                    | 2505         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 12288        |\n",
      "|    agent/train/approx_kl             | 0.0041706255 |\n",
      "|    agent/train/clip_fraction         | 0.194        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.39        |\n",
      "|    agent/train/explained_variance    | 0.967        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.00442     |\n",
      "|    agent/train/n_updates             | 50           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0122      |\n",
      "|    agent/train/std                   | 0.978        |\n",
      "|    agent/train/value_loss            | 0.124        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.19e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -5.74     |\n",
      "|    agent/time/fps                      | 2.5e+03   |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 1.23e+04  |\n",
      "|    agent/train/approx_kl               | 0.00324   |\n",
      "|    agent/train/clip_fraction           | 0.128     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.38     |\n",
      "|    agent/train/explained_variance      | 0.983     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.182     |\n",
      "|    agent/train/n_updates               | 60        |\n",
      "|    agent/train/policy_gradient_loss    | -0.00595  |\n",
      "|    agent/train/std                     | 0.962     |\n",
      "|    agent/train/value_loss              | 0.105     |\n",
      "|    preferences/entropy                 | 0.0106    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.969     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00451   |\n",
      "|    reward/epoch-0/train/loss           | 0.103     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.969     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00451   |\n",
      "|    reward/epoch-1/train/loss           | 0.105     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.969     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00451   |\n",
      "|    reward/epoch-2/train/loss           | 0.0971    |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.969     |\n",
      "|    final/train/gt_reward_loss          | 0.00451   |\n",
      "|    final/train/loss                    | 0.0971    |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'reward_loss': 0.0971360150059419, 'reward_accuracy': 0.96875}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pref_comparisons.train(\n",
    "    total_timesteps=5_000,\n",
    "    total_comparisons=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.rewards.reward_wrapper import RewardVecEnvWrapper\n",
    "\n",
    "learned_reward_venv = RewardVecEnvWrapper(venv, reward_net.predict_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7a2f0c7752d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = PPO(\n",
    "    seed=0,\n",
    "    policy=FeedForward32Policy,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "    ),\n",
    "    env=learned_reward_venv,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.01,\n",
    "    n_epochs=10,\n",
    "    n_steps=2048 // learned_reward_venv.num_envs,\n",
    "    clip_range=0.1,\n",
    "    gae_lambda=0.95,\n",
    "    gamma=0.97,\n",
    "    learning_rate=2e-3,\n",
    ")\n",
    "learner.learn(1_000)  # Note: set to 100_000 to train a proficient expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -1307 +/- 125\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "n_eval_episodes = 10\n",
    "reward_mean, reward_std = evaluate_policy(learner.policy, venv, n_eval_episodes)\n",
    "reward_stderr = reward_std / np.sqrt(n_eval_episodes)\n",
    "print(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
